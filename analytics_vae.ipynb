{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QpLCRNRfQ93I",
        "6fc0keYxY6Cu",
        "X0hSndjkY0Ey"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5796eb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7148f0c1-ac9e-4887-f63a-be48c5ba44f1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a90c3ff2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3f99170-3ccb-4ccc-c97a-efe7e7aa3671"
      },
      "source": [
        "%cd /content/drive/MyDrive/master/ivae"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/master/ivae\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNqQhTn0O82s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "6ac2b7be-553e-429f-8b77-d700ce9e9e82"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'models'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2954585022.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# --- Imports from your project ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Adjust if you keep everything in one file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miVAETrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0melbo_loss\u001b[0m   \u001b[0;31m# or import from your training script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# ============================================\n",
        "# analysis.py â€” Compare VAE vs iVAE-trained\n",
        "# Reconstruction error (MSE) over iterative inference\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Imports from your project ---\n",
        "# Adjust if you keep everything in one file\n",
        "from models import VAE, Encoder, Decoder, iVAETrainer\n",
        "from losses import elbo_loss   # or import from your training script\n",
        "from trainer import get_dataloaders, device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MSE"
      ],
      "metadata": {
        "id": "QpLCRNRfQ93I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Utility: Mean Squared Error (MSE) metric\n",
        "# ============================================\n",
        "def recon_mse(recon, target):\n",
        "    \"\"\"\n",
        "    Computes per-sample MSE between reconstructions and targets.\n",
        "    recon, target: [B, D] in [0,1]\n",
        "    Returns:\n",
        "        mse_per_sample: numpy array [B]\n",
        "        mean_mse: scalar\n",
        "    \"\"\"\n",
        "    mse_per_sample = F.mse_loss(recon, target, reduction='none').mean(dim=1)\n",
        "    return mse_per_sample.cpu().numpy(), float(mse_per_sample.mean())"
      ],
      "metadata": {
        "id": "i_euVIWNQ5sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#model"
      ],
      "metadata": {
        "id": "6fc0keYxY6Cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Model loader\n",
        "# ============================================\n",
        "def load_models(cfg, device):\n",
        "    \"\"\"\n",
        "    Loads trained VAE and iVAE models from disk.\n",
        "    Returns:\n",
        "        vae, ivae_trainer, ivae_encoder, ivae_decoder\n",
        "    \"\"\"\n",
        "    # --- Load baseline VAE ---\n",
        "    vae = VAE(z_dim=cfg['z_dim']).to(device)\n",
        "    vae_ckpt = torch.load(os.path.join(cfg['save_dir'], \"vae_baseline.pth\"), map_location=device)\n",
        "    vae.load_state_dict(vae_ckpt)\n",
        "\n",
        "    # --- Load iVAE checkpoint ---\n",
        "    ivae_ckpt = torch.load(os.path.join(cfg['save_dir'], \"ivae.pth\"), map_location=device)\n",
        "    ivae_enc = Encoder(z_dim=cfg['z_dim']).to(device)\n",
        "    ivae_dec = Decoder(z_dim=cfg['z_dim']).to(device)\n",
        "    ivae_enc.load_state_dict(ivae_ckpt[\"encoder\"])\n",
        "    ivae_dec.load_state_dict(ivae_ckpt[\"decoder\"])\n",
        "\n",
        "    ivae_trainer = iVAETrainer(\n",
        "        ivae_enc, ivae_dec,\n",
        "        latent_dim=cfg['z_dim'],\n",
        "        beta=cfg['beta'],\n",
        "        lr_model=cfg['lr_model'],\n",
        "        lr_inf=cfg['lr_inf'],\n",
        "        device=device,\n",
        "        use_amp=False\n",
        "    )\n",
        "\n",
        "    return vae, ivae_trainer, ivae_enc, ivae_dec"
      ],
      "metadata": {
        "id": "BsAW3iYERFCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iterative inference evaluation"
      ],
      "metadata": {
        "id": "liQL-WrEViqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iterative_recon_mse(encoder, decoder, x, n_steps, lr_eval, device, beta,\n",
        "                        update_decoder=False, save_path=None, save_latent_traj=False):\n",
        "    \"\"\"\n",
        "    Run iterative SVI refinement starting from amortized encoder outputs.\n",
        "    Optionally fine-tune decoder, and record latent evolution.\n",
        "    Returns:\n",
        "        mses_over_steps: numpy array [n_steps+1]\n",
        "        recons_list: list of recon tensors (optional)\n",
        "        z_traj: list of latent means per iteration [n_steps+1, B, z_dim] if save_latent_traj=True\n",
        "    \"\"\"\n",
        "    encoder.eval()\n",
        "    decoder.train() if update_decoder else decoder.eval()\n",
        "    x = x.to(device)\n",
        "\n",
        "    opt_dec = torch.optim.Adam(decoder.parameters(), lr=lr_eval) if update_decoder else None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mu0, logvar0 = encoder(x)\n",
        "\n",
        "    mu = mu0.clone().detach().to(device).requires_grad_(True)\n",
        "    logvar = logvar0.clone().detach().to(device).requires_grad_(True)\n",
        "\n",
        "    mses, recons = [], []\n",
        "    z_traj = [mu.detach().cpu().numpy()] if save_latent_traj else None\n",
        "\n",
        "    # Step 0 reconstruction\n",
        "    with torch.no_grad():\n",
        "        z = mu\n",
        "        x_logit = decoder(z)\n",
        "        x_recon = torch.sigmoid(x_logit)\n",
        "        recons.append(x_recon.detach().cpu())\n",
        "        _, mean_mse0 = recon_mse(x_recon.view(x_recon.size(0), -1),\n",
        "                                 x.view(x.size(0), -1))\n",
        "        mses.append(mean_mse0)\n",
        "\n",
        "    for t in range(1, n_steps + 1):\n",
        "        loss, _ = elbo_loss(x, decoder, mu, logvar, beta=beta)\n",
        "        grads = torch.autograd.grad(loss, [mu, logvar], retain_graph=False, create_graph=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mu -= lr_eval * grads[0]\n",
        "            logvar -= lr_eval * grads[1]\n",
        "            mu.requires_grad_(True)\n",
        "            logvar.requires_grad_(True)\n",
        "\n",
        "        # optional decoder fine-tuning\n",
        "        if update_decoder:\n",
        "            opt_dec.zero_grad()\n",
        "            loss.backward()\n",
        "            opt_dec.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x_logit = decoder(mu)\n",
        "            x_recon = torch.sigmoid(x_logit)\n",
        "            recons.append(x_recon.detach().cpu())\n",
        "            _, mean_mse = recon_mse(x_recon.view(x_recon.size(0), -1),\n",
        "                                    x.view(x.size(0), -1))\n",
        "            mses.append(mean_mse)\n",
        "\n",
        "        if save_latent_traj:\n",
        "            z_traj.append(mu.detach().cpu().numpy())\n",
        "\n",
        "    if update_decoder and save_path is not None:\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        torch.save(decoder.state_dict(), os.path.join(save_path, \"decoder_test.pth\"))\n",
        "        print(f\"[âœ“] Saved test-adapted decoder to {os.path.join(save_path, 'decoder_test.pth')}\")\n",
        "\n",
        "    if save_latent_traj:\n",
        "        return np.array(mses), recons, np.stack(z_traj)  # [steps+1, B, z_dim]\n",
        "    else:\n",
        "        return np.array(mses), recons\n"
      ],
      "metadata": {
        "id": "_jtg6thZQ6Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#plots and analytics"
      ],
      "metadata": {
        "id": "X0hSndjkY0Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Main evaluation function\n",
        "# ============================================\n",
        "def run_analysis(cfg, test_loader, device, n_examples=256, n_steps=50,\n",
        "                 lr_eval_factor=0.1, show_plot=True):\n",
        "    \"\"\"\n",
        "    Compare reconstruction MSE between:\n",
        "        1. VAE baseline (amortized)\n",
        "        2. iVAE-trained (iterative inference)\n",
        "    Returns:\n",
        "        mse_curves = [vae_mse_constant, ivae_mse_curve]\n",
        "    \"\"\"\n",
        "    # Load models\n",
        "    vae, ivae_trainer, ivae_enc, ivae_dec = load_models(cfg, device)\n",
        "\n",
        "    # Collect test samples\n",
        "    imgs = []\n",
        "    for x, _ in test_loader:\n",
        "        imgs.append(x)\n",
        "        if len(torch.cat(imgs)) >= n_examples:\n",
        "            break\n",
        "    imgs = torch.cat(imgs, dim=0)[:n_examples].to(device)\n",
        "    x_flat = imgs.view(imgs.size(0), -1)\n",
        "\n",
        "    # --- (1) VAE baseline amortized ---\n",
        "    vae.eval()\n",
        "    with torch.no_grad():\n",
        "        mu_vae, logvar_vae = vae.encoder(x_flat)\n",
        "        x_logit = vae.decoder(mu_vae)\n",
        "        x_recon = torch.sigmoid(x_logit)\n",
        "        _, vae_mse = recon_mse(x_recon.view(x_recon.size(0), -1),\n",
        "                               x_flat)\n",
        "    print(f\"VAE amortized reconstruction MSE: {vae_mse:.4f}\")\n",
        "\n",
        "    # --- (2) iVAE-trained iterative inference ---\n",
        "    lr_eval = ivae_trainer.lr_inf * lr_eval_factor\n",
        "    ivae_mses, _ = iterative_recon_mse(\n",
        "        ivae_enc, ivae_dec, x_flat, n_steps, lr_eval, device, beta=cfg['beta']\n",
        "    )\n",
        "    print(f\"iVAE MSE after {n_steps} steps: {ivae_mses[-1]:.4f}\")\n",
        "\n",
        "    # --- Aggregate results ---\n",
        "    vae_constant = np.ones_like(ivae_mses) * vae_mse\n",
        "    mse_curves = [vae_constant, ivae_mses]\n",
        "\n",
        "    # --- Optional Plot ---\n",
        "    if show_plot:\n",
        "        iters = np.arange(0, n_steps + 1)\n",
        "        plt.figure(figsize=(7, 5))\n",
        "        plt.plot(iters, vae_constant, '--', label='VAE (amortized baseline)')\n",
        "        plt.plot(iters, ivae_mses, label='iVAE (iterative refinement)')\n",
        "        plt.xlabel(\"SVI Iteration\")\n",
        "        plt.ylabel(\"Reconstruction MSE (â†“)\")\n",
        "        plt.title(\"Reconstruction Error vs SVI Iterations\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.gca().invert_yaxis()  # lower is better\n",
        "        plt.show()\n",
        "\n",
        "    return mse_curves\n",
        "\n"
      ],
      "metadata": {
        "id": "V3ULgrQTUWRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_latent_evolution(decoder, z_traj, x_true=None, num_samples=5, num_steps_to_show=5):\n",
        "    \"\"\"\n",
        "    Visualize decoded reconstructions along the latent refinement trajectory.\n",
        "    z_traj: numpy array [steps+1, B, z_dim]\n",
        "    Shows grid (num_samples x num_steps_to_show) of decoded images.\n",
        "    \"\"\"\n",
        "    steps, B, zdim = z_traj.shape\n",
        "    step_idxs = np.linspace(0, steps-1, num_steps_to_show, dtype=int)\n",
        "    num_samples = min(num_samples, B)\n",
        "\n",
        "    fig, axes = plt.subplots(num_samples, num_steps_to_show+1, figsize=(2.2*(num_steps_to_show+1), 2*num_samples))\n",
        "\n",
        "    decoder.eval()\n",
        "    for i in range(num_samples):\n",
        "        for j, step in enumerate(step_idxs):\n",
        "            z = torch.tensor(z_traj[step, i], dtype=torch.float32).unsqueeze(0).to(next(decoder.parameters()).device)\n",
        "            with torch.no_grad():\n",
        "                x_recon = torch.sigmoid(decoder(z)).cpu().view(28, 28)\n",
        "            axes[i, j].imshow(x_recon, cmap='gray', vmin=0, vmax=1)\n",
        "            axes[i, j].axis('off')\n",
        "            if i == 0:\n",
        "                axes[i, j].set_title(f\"iter {step}\")\n",
        "        # optionally show ground truth\n",
        "        if x_true is not None:\n",
        "            axes[i, -1].imshow(x_true[i].view(28, 28), cmap='gray', vmin=0, vmax=1)\n",
        "            axes[i, -1].axis('off')\n",
        "            if i == 0:\n",
        "                axes[i, -1].set_title(\"target\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "eu6ZqfC-YpQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "Mvy7G96tYvFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Example usage (Colab cell)\n",
        "# ============================================\n",
        "if __name__ == \"__main__\":\n",
        "    from config import CONFIG  # or define inline\n",
        "    _, test_loader = get_dataloaders(CONFIG)\n",
        "    curves = run_analysis(CONFIG, test_loader, device, n_examples=256, n_steps=50)"
      ],
      "metadata": {
        "id": "8_6wrNGaUd2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run iterative inference and record latents\n",
        "ivae_mses, _, z_traj = iterative_recon_mse(\n",
        "    ivae_enc, ivae_dec, x_flat[:5], n_steps=cfg['svi_steps_eval']//10,\n",
        "    lr_eval=ivae_trainer.lr_inf*0.1, device=device, beta=cfg['beta'],\n",
        "    save_latent_traj=True\n",
        ")\n",
        "\n",
        "# Plot decoded trajectory evolution\n",
        "plot_latent_evolution(ivae_dec, z_traj, x_true=x_flat[:5].cpu(), num_samples=5, num_steps_to_show=5)\n"
      ],
      "metadata": {
        "id": "KE0R_vMTYuKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Out-of-distribution"
      ],
      "metadata": {
        "id": "xOc02zU_de_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import functional as TF\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "# ================================================\n",
        "# 1. CORRUPTION FUNCTIONS\n",
        "# ================================================\n",
        "def add_white_noise(x, sigma=0.6):\n",
        "    \"\"\"Add Gaussian white noise to input [B,1,28,28].\"\"\"\n",
        "    noise = torch.randn_like(x) * sigma\n",
        "    x_noisy = x + noise\n",
        "    return torch.clamp(x_noisy, 0.0, 1.0)\n",
        "\n",
        "def add_gaussian_blur(x, sigma=2.0):\n",
        "    \"\"\"Apply Gaussian blur (approximation using OpenCV per sample).\"\"\"\n",
        "    x_np = x.cpu().numpy()\n",
        "    x_blur = []\n",
        "    for i in range(x_np.shape[0]):\n",
        "        img = (x_np[i,0]*255).astype(np.uint8)\n",
        "        img_blur = cv2.GaussianBlur(img, (5,5), sigma)\n",
        "        x_blur.append(img_blur / 255.0)\n",
        "    x_blur = torch.tensor(np.stack(x_blur), dtype=torch.float32).unsqueeze(1)\n",
        "    return x_blur.to(x.device)\n",
        "\n",
        "def add_salt_pepper_noise(x, p=0.4):\n",
        "    \"\"\"Apply salt & pepper noise with probability p.\"\"\"\n",
        "    x_np = x.cpu().numpy()\n",
        "    noisy_imgs = []\n",
        "    for i in range(x_np.shape[0]):\n",
        "        img = x_np[i,0].copy()\n",
        "        mask = np.random.rand(*img.shape)\n",
        "        img[mask < (p/2)] = 0.0\n",
        "        img[mask > (1 - p/2)] = 1.0\n",
        "        noisy_imgs.append(img)\n",
        "    noisy_imgs = torch.tensor(np.stack(noisy_imgs), dtype=torch.float32).unsqueeze(1)\n",
        "    return noisy_imgs.to(x.device)\n",
        "\n",
        "# ================================================\n",
        "# 2. EVALUATION LOOP\n",
        "# ================================================\n",
        "def eval_corrupted_images(model_enc, model_dec, test_imgs, corruption_fn, corr_name,\n",
        "                          n_steps, lr_eval, beta, device, return_recons=False):\n",
        "    \"\"\"Apply corruption, run iterative inference, compute MSE.\"\"\"\n",
        "    x_corr = corruption_fn(test_imgs)\n",
        "    x_flat = x_corr.view(x_corr.size(0), -1)\n",
        "    accs, recons = iterative_recon_mse(\n",
        "        model_enc, model_dec, x_flat, n_steps=n_steps, lr_eval=lr_eval,\n",
        "        device=device, beta=beta, save_latent_traj=False\n",
        "    )\n",
        "    final_mse = accs[-1]\n",
        "    if return_recons:\n",
        "        return final_mse, x_corr, recons\n",
        "    return final_mse, x_corr\n",
        "\n",
        "# ================================================\n",
        "# 3. VISUALIZATION HELPERS\n",
        "# ================================================\n",
        "def plot_reconstruction_timeline(decoder, z_traj, x_true, x_corr, corruption_label,\n",
        "                                 num_steps_to_show=5):\n",
        "    \"\"\"Plot one sample's reconstruction evolution given corrupted input.\"\"\"\n",
        "    steps = len(z_traj)\n",
        "    step_idxs = np.linspace(0, steps-1, num_steps_to_show, dtype=int)\n",
        "    fig, axes = plt.subplots(1, num_steps_to_show+2, figsize=(2.5*(num_steps_to_show+2), 3))\n",
        "\n",
        "    decoder.eval()\n",
        "    for j, step in enumerate(step_idxs):\n",
        "        z = torch.tensor(z_traj[step], dtype=torch.float32).unsqueeze(0).to(next(decoder.parameters()).device)\n",
        "        with torch.no_grad():\n",
        "            x_recon = torch.sigmoid(decoder(z)).cpu().view(28, 28)\n",
        "        axes[j+1].imshow(x_recon, cmap='gray', vmin=0, vmax=1)\n",
        "        axes[j+1].axis('off')\n",
        "        axes[j+1].set_title(f\"iter {step}\")\n",
        "\n",
        "    axes[0].imshow(x_corr.cpu().view(28,28), cmap='gray', vmin=0, vmax=1)\n",
        "    axes[0].axis('off')\n",
        "    axes[0].set_title(f\"Input ({corruption_label})\")\n",
        "\n",
        "    axes[-1].imshow(x_true.cpu().view(28,28), cmap='gray', vmin=0, vmax=1)\n",
        "    axes[-1].axis('off')\n",
        "    axes[-1].set_title(\"Target\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_ood_accuracy_bars(vae_scores, ivae_scores, labels):\n",
        "    \"\"\"Bar plot comparing VAE vs iVAE mean MSE (lower is better).\"\"\"\n",
        "    vae_means = [np.mean(s) for s in vae_scores]\n",
        "    vae_stds = [np.std(s) for s in vae_scores]\n",
        "    ivae_means = [np.mean(s) for s in ivae_scores]\n",
        "    ivae_stds = [np.std(s) for s in ivae_scores]\n",
        "\n",
        "    x = np.arange(len(labels))\n",
        "    width = 0.35\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8,5))\n",
        "    ax.bar(x - width/2, vae_means, width, yerr=vae_stds, label='VAE', capsize=4)\n",
        "    ax.bar(x + width/2, ivae_means, width, yerr=ivae_stds, label='iVAE', capsize=4)\n",
        "\n",
        "    ax.set_ylabel('Reconstruction MSE â†“')\n",
        "    ax.set_title('Reconstruction error across corruption types')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.4)\n",
        "    plt.show()\n",
        "\n",
        "# ================================================\n",
        "# 4. OOD TEST DRIVER\n",
        "# ================================================\n",
        "def run_ood_analysis(cfg, vae, ivae_enc, ivae_dec, test_loader, device):\n",
        "    vae.eval(); ivae_enc.eval(); ivae_dec.eval()\n",
        "\n",
        "    # sample a batch\n",
        "    x, _ = next(iter(test_loader))\n",
        "    x = x.to(device)\n",
        "    x_flat = x.view(x.size(0), -1)\n",
        "\n",
        "    n_steps = cfg['svi_steps_eval']\n",
        "    lr_eval = cfg['lr_inf'] * 0.1\n",
        "\n",
        "    # --- Corruptions ---\n",
        "    corruption_tasks = [\n",
        "        (\"Vanilla\", lambda x: x),\n",
        "        (\"Blur Ïƒ=2\", lambda x: add_gaussian_blur(x, sigma=2.0)),\n",
        "        (\"White noise Ïƒ=0.6\", lambda x: add_white_noise(x, sigma=0.6)),\n",
        "        (\"Salt&Pepper p=0.4\", lambda x: add_salt_pepper_noise(x, p=0.4)),\n",
        "    ]\n",
        "\n",
        "    vae_scores = []\n",
        "    ivae_scores = []\n",
        "\n",
        "    for name, corr_fn in corruption_tasks:\n",
        "        print(f\"Testing corruption: {name}\")\n",
        "\n",
        "        # --- VAE baseline ---\n",
        "        with torch.no_grad():\n",
        "            x_corr = corr_fn(x)\n",
        "            mu_vae, logvar_vae = vae.encoder(x_corr.view(x_corr.size(0), -1))\n",
        "            recon = torch.sigmoid(vae.decoder(mu_vae))\n",
        "            _, mse_vae = recon_mse(recon, x_corr.view(x_corr.size(0), -1))\n",
        "        vae_scores.append([mse_vae])\n",
        "\n",
        "        # --- iVAE iterative ---\n",
        "        mse_ivae, _, _ = iterative_recon_mse(\n",
        "            ivae_enc, ivae_dec, x_corr.view(x_corr.size(0), -1),\n",
        "            n_steps=n_steps, lr_eval=lr_eval, device=device, beta=cfg['beta'],\n",
        "            save_latent_traj=False\n",
        "        )\n",
        "        ivae_scores.append([mse_ivae[-1]])\n",
        "\n",
        "    # --- Visualization: comparison bar plot ---\n",
        "    plot_ood_accuracy_bars(vae_scores, ivae_scores,\n",
        "                           [\"Vanilla\", \"Blur\", \"WhiteNoise\", \"Salt&Pepper\"])\n",
        "\n",
        "    # --- Timeline reconstruction example ---\n",
        "    print(\"\\nVisualizing reconstruction timeline for one sample per corruption...\")\n",
        "    for name, corr_fn in corruption_tasks[1:]:\n",
        "        x_corr = corr_fn(x[:1])\n",
        "        _, _, z_traj = iterative_recon_mse(\n",
        "            ivae_enc, ivae_dec, x_corr.view(1, -1),\n",
        "            n_steps=n_steps//10, lr_eval=lr_eval,\n",
        "            device=device, beta=cfg['beta'],\n",
        "            save_latent_traj=True\n",
        "        )\n",
        "        plot_reconstruction_timeline(\n",
        "            ivae_dec, z_traj[:,0,:], x_true=x[0], x_corr=x_corr[0],\n",
        "            corruption_label=name, num_steps_to_show=5\n",
        "        )\n"
      ],
      "metadata": {
        "id": "5tNyOnNZdjJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q78d4GUPdj3R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}