{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00cc4c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python + PyTorch dependencies for data loading/visualization.\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Imaging utilities leveraged elsewhere in the notebook.\n",
    "from diffusers import AutoencoderKL\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fc8479",
   "metadata": {},
   "source": [
    "# image pipiline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7798dae3",
   "metadata": {},
   "source": [
    "## image load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11097703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset path\n",
    "dataset_path = \"/scratch/juriostegui/sun397-subset/test/\"\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Normalize images to SD training resolution.\n",
    "    transforms.ToTensor()  # Convert PIL images to torch tensors in [0, 1].\n",
    "])\n",
    "\n",
    "# Load dataset from folder\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "# Build an iterator that returns random single-image batches for masking.\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a94d5",
   "metadata": {},
   "source": [
    "## image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f1c3b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create random square mask\n",
    "def random_square_mask(img_tensor):\n",
    "    \"\"\"Randomly removes a square region and returns the masked image plus binary mask.\"\"\"\n",
    "    _, H, W = img_tensor.shape  # Track spatial size to sample a crop.\n",
    "    h = H // 2  # Use half the height to form a reasonably large hole.\n",
    "    w = W // 2  # Mirror logic for width to keep the square aspect ratio.\n",
    "    top = random.randint(0, H - h)  # Sample top-left corner uniformly.\n",
    "    left = random.randint(0, W - w)\n",
    "    mask = torch.zeros(1, H, W, device=img_tensor.device)  # Binary mask initialized to background.\n",
    "    mask[:, top:top+h, left:left+w] = 1.0  # Mark the missing region with ones.\n",
    "\n",
    "    masked_img = img_tensor.clone()  # Work on a copy to preserve the original tensor.\n",
    "    masked_img[:, top:top+h, left:left+w] = 0.0  # Zero out the pixels we want the model to inpaint.\n",
    "    return masked_img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1821fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 5 random images and apply making\n",
    "images = []  # Unmasked reference images for visualization.\n",
    "masks = []  # Binary masks storing which pixels are missing.\n",
    "masked_images = []  # Observations with central chunks removed.\n",
    "\n",
    "for i, (img, _) in enumerate(data_loader):\n",
    "    if i >= 5:\n",
    "        break  # Limit to a small batch for interactive experimentation.\n",
    "    img = img[0].cuda()  # Move the selected image to GPU for masking.\n",
    "    masked_img, mask = random_square_mask(img)  # Apply the stochastic mask.\n",
    "    images.append(img.cpu())  # Store CPU copies for plotting later.\n",
    "    masked_images.append(masked_img.cpu())\n",
    "    masks.append(mask.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b09dd6",
   "metadata": {},
   "source": [
    "## get latent mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6eba15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_mask_latents(mask_batch: torch.Tensor, target_hw):\n",
    "    \"\"\"Resize binary masks to latent space so we can reinsert known pixels per Song et al.\"\"\"\n",
    "    if mask_batch.ndim == 3:\n",
    "        mask_batch = mask_batch.unsqueeze(1)  # Ensure a channel dimension for interpolation.\n",
    "    mask = mask_batch.to(device=device, dtype=sd_dtype)\n",
    "    mask = F.interpolate(mask, size=target_hw, mode=\"nearest\")  # Downsample mask without smoothing edges.\n",
    "    known_mask = 1.0 - mask  # Known pixels are the complement of the missing region.\n",
    "    return known_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd88080",
   "metadata": {},
   "source": [
    "## Stable Diffusion predictor-corrector inpainter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d2e07da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429aa3f78574492395d9980f0ca88b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from typing import Optional\n",
    "import math\n",
    "\n",
    "sd_model_id = \"runwayml/stable-diffusion-v1-5\"  # Use the base SD checkpoint for general content.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Respect accelerator availability.\n",
    "sd_dtype = torch.float32  # Default precision keeps things simple across hardware.\n",
    "\n",
    "# Instantiate the Stable Diffusion pipeline without relying on the specialized inpaint model.\n",
    "sd_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    sd_model_id,\n",
    "    torch_dtype=sd_dtype\n",
    ")\n",
    "sd_pipe.safety_checker = None  # Disable safety checker to avoid extra overhead in research.\n",
    "sd_pipe.scheduler = DDIMScheduler.from_config(sd_pipe.scheduler.config)  # Use DDIM for Song-style updates.\n",
    "sd_pipe = sd_pipe.to(device)  # Move UNet/VAE/text encoder to the working device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d95dc",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e7f9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def _encode_to_latents(images: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Project masked RGB images into the VAE latent space used during SD training.\"\"\"\n",
    "    images = images.to(device=device, dtype=sd_dtype)\n",
    "    images = images * 2.0 - 1.0  # Map from [0, 1] to the VAE's expected [-1, 1] range.\n",
    "    latents = sd_pipe.vae.encode(images).latent_dist.mean  # Use the posterior mean for determinism.\n",
    "    latents = latents * sd_pipe.vae.config.get(\"scaling_factor\", 0.18215)  # Match SD latent scaling.\n",
    "    return latents\n",
    "\n",
    "\n",
    "\n",
    "def _blend_with_observation(latents, observed_latents, known_mask, timestep, generator):\n",
    "    \"\"\"Inject noisy observations for known pixels, copying Song's conditioning strategy.\"\"\"\n",
    "    if known_mask is None:\n",
    "        return latents\n",
    "    noise = torch.randn_like(observed_latents, generator=generator, device=device, dtype=observed_latents.dtype)\n",
    "    noised_obs = sd_pipe.scheduler.add_noise(observed_latents, noise, timestep)  # Forward-diffuse observations.\n",
    "    return latents * (1.0 - known_mask) + noised_obs * known_mask  # Replace only the constrained regions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9371f1",
   "metadata": {},
   "source": [
    "## predictor/corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4547f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict_noise(latents, timestep, text_embeddings, guidance_scale, do_cfg):\n",
    "    \"\"\"Run the SD UNet and optionally apply classifier-free guidance for text alignment.\"\"\"\n",
    "    latent_input = torch.cat([latents] * 2) if do_cfg else latents  # Duplicate for conditional/unconditional passes.\n",
    "    latent_input = sd_pipe.scheduler.scale_model_input(latent_input, timestep)  # Match scheduler expected scaling.\n",
    "    noise_pred = sd_pipe.unet(latent_input, timestep, encoder_hidden_states=text_embeddings).sample\n",
    "    if do_cfg:\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        return noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)  # CFG blend.\n",
    "    return noise_pred\n",
    "\n",
    "\n",
    "def _predictor_step(latents, timestep, text_embeddings, guidance_scale, do_cfg):\n",
    "    \"\"\"Single DDIM predictor step that integrates the reverse SDE/ODE.\"\"\"\n",
    "    noise_pred = _predict_noise(latents, timestep, text_embeddings, guidance_scale, do_cfg)\n",
    "    latents = sd_pipe.scheduler.step(noise_pred, timestep, latents).prev_sample  # Deterministic DDIM update.\n",
    "    return latents\n",
    "\n",
    "\n",
    "def _corrector_step(latents, timestep, text_embeddings, guidance_scale, snr, generator, do_cfg):\n",
    "    \"\"\"Langevin corrector step that injects noise according to the desired SNR.\"\"\"\n",
    "    score = _predict_noise(latents, timestep, text_embeddings, guidance_scale, do_cfg)\n",
    "    noise = torch.randn_like(latents, generator=generator, device=device, dtype=latents.dtype)\n",
    "    score_norm = torch.norm(score.reshape(score.shape[0], -1), dim=-1).mean().item()\n",
    "    noise_norm = torch.norm(noise.reshape(noise.shape[0], -1), dim=-1).mean().item()\n",
    "    step_size = (snr ** 2) * ((noise_norm / (score_norm + 1e-12)) ** 2)  # Follow Song's adaptive step sizing.\n",
    "    latents = latents + step_size * score + math.sqrt(2.0 * step_size) * noise  # Euler-Maruyama update.\n",
    "    return latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42529373",
   "metadata": {},
   "source": [
    "## inpainting pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5642eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sd_pc_inpainter(pipe: StableDiffusionPipeline,\n",
    "                        snr: float = 0.15,\n",
    "                        n_corrector_steps: int = 1,\n",
    "                        denoise: bool = True):\n",
    "    \"\"\"Construct a Song-style predictor-corrector inpainting callable for Stable Diffusion.\"\"\"\n",
    "\n",
    "    def pc_inpainter(masked_images: torch.Tensor,\n",
    "                    masks: torch.Tensor,\n",
    "                    prompt: str = \"cat\",\n",
    "                    negative_prompt: Optional[str] = \"cat\",\n",
    "                    num_inference_steps: int = 30,\n",
    "                    guidance_scale: float = 0,\n",
    "                    generator: Optional[torch.Generator] = None):\n",
    "        \"\"\"Run the predictor-corrector loop conditioned on text + observed pixels.\"\"\"\n",
    "        pipe.scheduler.set_timesteps(num_inference_steps, device=device)  # Reset DDIM schedule each call.\n",
    "        masked_images = masked_images.to(device=device, dtype=sd_dtype)\n",
    "        masks = masks.to(device=device, dtype=sd_dtype)\n",
    "        batch_size, _, height, width = masked_images.shape\n",
    "        latent_h, latent_w = height // 8, width // 8  # Latent resolution is 1/8th of pixel space.\n",
    "        if generator is None:\n",
    "            generator = torch.Generator(device=device)  # Allow deterministic seeding upstream if desired.\n",
    "\n",
    "        latents = torch.randn((batch_size, pipe.unet.config.in_channels, latent_h, latent_w),\n",
    "                              generator=generator,\n",
    "                              device=device,\n",
    "                              dtype=sd_dtype)\n",
    "        latents = latents * pipe.scheduler.init_noise_sigma  # Match scheduler noise std.\n",
    "        observed_latents = _encode_to_latents(masked_images)  # Encode partial observations once upfront.\n",
    "        known_mask = _prepare_mask_latents(masks, (latent_h, latent_w))\n",
    "        negative_prompt_text = negative_prompt or \" cat\"\n",
    "        prompt_text = prompt or \" cat\"  # Fall back to empty strings when users skip text guidance.\n",
    "        do_cfg = guidance_scale > 1.0 and bool(prompt_text.strip())  # Only enable CFG when we have a prompt.\n",
    "        if not do_cfg:\n",
    "            text_embeddings = pipe._encode_prompt(\n",
    "                prompt_text,\n",
    "                device=device,\n",
    "                num_images_per_prompt=1,\n",
    "                do_classifier_free_guidance=False,\n",
    "                negative_prompt=negative_prompt_text\n",
    "            )\n",
    "        else:\n",
    "            text_embeddings = pipe._encode_prompt(\n",
    "                prompt_text,\n",
    "                device=device,\n",
    "                num_images_per_prompt=1,\n",
    "                do_classifier_free_guidance=True,\n",
    "                negative_prompt=negative_prompt_text\n",
    "            )\n",
    "\n",
    "        for timestep in pipe.scheduler.timesteps:\n",
    "            for _ in range(n_corrector_steps):\n",
    "                latents = _corrector_step(latents, timestep, text_embeddings, guidance_scale, snr, generator, do_cfg)\n",
    "                latents = _blend_with_observation(latents, observed_latents, known_mask, timestep, generator)  # Enforce constraints.\n",
    "            latents = _predictor_step(latents, timestep, text_embeddings, guidance_scale, do_cfg)\n",
    "            latents = _blend_with_observation(latents, observed_latents, known_mask, timestep, generator)\n",
    "\n",
    "        if denoise:\n",
    "            latents = _predictor_step(latents, pipe.scheduler.timesteps[-1], text_embeddings, guidance_scale, do_cfg)  # Optional final clean-up.\n",
    "\n",
    "        images = pipe.vae.decode(latents / pipe.vae.config.get(\"scaling_factor\", 0.18215)).sample\n",
    "        images = (images / 2 + 0.5).clamp(0, 1)  # Return images in displayable range.\n",
    "        return images\n",
    "\n",
    "    return pc_inpainter\n",
    "\n",
    "\n",
    "pc_inpainter = get_sd_pc_inpainter(sd_pipe, snr=0.2, n_corrector_steps=0, denoise=True)  # Build the callable with default research settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1da333f",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "778ffb06",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m masked_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(masked_images)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m mask_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(masks)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m recon \u001b[38;5;241m=\u001b[39m \u001b[43mpc_inpainter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasked_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasked_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 34\u001b[0m, in \u001b[0;36mget_sd_pc_inpainter.<locals>.pc_inpainter\u001b[0;34m(masked_images, masks, prompt, negative_prompt, num_inference_steps, guidance_scale, generator)\u001b[0m\n\u001b[1;32m     32\u001b[0m do_cfg \u001b[38;5;241m=\u001b[39m guidance_scale \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(prompt_text\u001b[38;5;241m.\u001b[39mstrip())  \u001b[38;5;66;03m# Only enable CFG when we have a prompt.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m do_cfg:\n\u001b[0;32m---> 34\u001b[0m     text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_images_per_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt_text\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     text_embeddings \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39m_encode_prompt(\n\u001b[1;32m     43\u001b[0m         prompt_text,\n\u001b[1;32m     44\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m         negative_prompt\u001b[38;5;241m=\u001b[39mnegative_prompt_text\n\u001b[1;32m     48\u001b[0m     )\n",
      "File \u001b[0;32m~/work/env/lib/python3.11/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:328\u001b[0m, in \u001b[0;36mStableDiffusionPipeline._encode_prompt\u001b[0;34m(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt, prompt_embeds, negative_prompt_embeds, lora_scale, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m prompt_embeds_tuple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_prompt(\n\u001b[1;32m    316\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m    317\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    325\u001b[0m )\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# concatenate for backwards comp\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m prompt_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt_embeds_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_embeds_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt_embeds\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got NoneType"
     ]
    }
   ],
   "source": [
    "# Example usage (do not execute on shared cluster login nodes).\n",
    "generator = torch.Generator(device=device).manual_seed(0)\n",
    "masked_batch = torch.stack(masked_images).to(device)\n",
    "mask_batch = torch.stack(masks).to(device)\n",
    "recon = pc_inpainter(\n",
    "    masked_images=masked_batch,\n",
    "    masks=mask_batch,\n",
    "    num_inference_steps=40,\n",
    "    generator=generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aa4259",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f503202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, len(masked_images), figsize=(15, 6))\n",
    "for idx in range(len(masked_images)):\n",
    "    axes[0, idx].imshow(images[idx].permute(1, 2, 0))\n",
    "    axes[0, idx].set_title(\"Original\")\n",
    "    axes[0, idx].axis(\"off\")\n",
    "    axes[1, idx].imshow(masked_images[idx].permute(1, 2, 0))\n",
    "    axes[1, idx].set_title(\"Masked\")\n",
    "    axes[1, idx].axis(\"off\")\n",
    "    axes[2, idx].imshow(recon[idx].detach().cpu().permute(1, 2, 0))\n",
    "    axes[2, idx].set_title(\"Inpainted\")\n",
    "    axes[2, idx].axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (latent-imagination)",
   "language": "python",
   "name": "latent-imagination"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
